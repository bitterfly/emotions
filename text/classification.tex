\documentclass[main.tex]{subfiles}

\begin{document}
\section{Класификация}
\label{chap:em}

След като сме избрали характеристичните вектори, които ще извличаме по подаден wav файл, трябва да можем да ги класифицираме по някакъв начин.
В проучването \cite{survey} са разгледани и сравнени различни методи за класификация. Макар че невронните мрежи са по-често използвани в последните публикации в областта,
тук ще се подходи по ,,старомодния'' начин с Гаусови смески. Показаното за тях съотношение между ,,прецизност на разпознаване'' и ,,време за трениране'' е най-добро, според проучването.

Целим да намерим разпределение за всяка от търсените емоции. Всяко непрекъснато разпределение върху $\mathbb{R}^n$ може да се приближи с произволна точност с линейна комбинация на достатъчно на брой гаусиани, където теглата се сумират до 1. Такава сума ще наричаме Гаусова смеска. 

Нека за всяка емоция $e$ сме приближили разпределението на векторите ѝ със смеска от $K$ на брой гаусиани. Нека означим тази смеска с $(\pi^e, \mu^e, \Sigma^e)$, където:
\begin{flalign*}
    & \pi^e = \{\pi_k^e\}_{k=1}^K, \text{тегла} & \\
    & \mu^e = \{\mu_k^e\}_{k=1}^K & \\
    & \Sigma^e = \{\Sigma_k^e\}_{k=1}^K &
\end{flalign*}

Тогава при подаден нов характеристичен вектор $x$, ще търсим смеската на коя емоция ще доведе до най-голяма вероятностна плътност (до най-голямо правдоподобие) - тоест параметрите на кой модел е най-вероятно да са генерирали наблюдението. При подадени $(\pi^e, \mu^e, \Sigma^e)$ за дадена емоция $e$ и характеристичен вектор $x$, вероятностната плътност на смеската се пресмята с формулата:

$p(x) = \sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x; \mu_k^e, \Sigma_k^e)$,

където $\mathcal{N}(\bullet, \mu, \Sigma)$ е плътност на нормално разпределение със средно $\mu$ и ковариационна матрица $\Sigma$, $\mu_i^e \in \mathbb{R}^{m}$, а $\sum\limits_{k=1}^K \pi_k^e = 1; 0\leq \pi_i^e\leq 1$ и в случая на избраните характеристични вектори, $m=39$.

По принцип ковариационната матрица $\Sigma_i^e \in (\mathbb{R}^{m \times m})$, но ако приемем, че отделните MFCC коефициенти са независими, то $\Sigma_i^e$ ще бъде диагонална. За да се намалят параметрите на модела, ще приемем, че това е така.

Алгоритъмът за получаване на въпросните $\pi^e, \mu^e, \Sigma^e$ е следният:

Нека $X=(x_1,\ldots,x_n)$ са всички характеристични вектори с етикет $e$. Искаме правдоподобието на тези вектори, спрямо $(\pi^e, \mu^e, \Sigma^e)$ да е възможно най-голямо. Тоест искаме да оптимизираме:

$p(X|(\pi^e, \mu^e, \Sigma^e)) = \prod\limits_{i=1}^{n} \sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x_i; \mu_k^e, \Sigma_k^e)$.

Тъй като логаритъмът е монотонно растяща функция, то няма значение дали ще оптимизираме функцията със или без логаритъм. За по-голямо удобство, нека разглеждаме $\log(p(X|(\pi^e, \mu^e, \Sigma^e)))$. Тоест имаме:
\begin{flalign*}
    & \log(p(X|(\pi^e, \mu^e, \Sigma^e))) = log(\prod\limits_{i=1}^{n} \sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x_i; \mu_k^e, \Sigma_k^e)) = \sum\limits_{i=1}^{n} \log(\sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x_i; \mu_k^e, \Sigma_k^e))) &
\end{flalign*}
Нека сме фиксирали някаква емоция. За удобство ще означаваме Гаусовата ѝ смеска с $(\pi, \mu, \Sigma)$. Оптимизационната задача трябва да отчита ограниченията за $\pi$, затова след добавяне на множител на Лагранж има вида:

$L(\pi, \mu, \Sigma) = \sum\limits_{i=1}^{n} \log(\sum\limits_{k=1}^{K} \pi_k \mathcal{N}(x_i; \mu_k, \Sigma_k))) + \lambda(\sum\limits_{k=1}^K \pi_k - 1)$

За да максимизираме правдоподобието, търсим решения на \\$\cfrac{\partial L(\pi, \mu, \Sigma)}{\partial \mu_j} = 0, \cfrac{\partial L(\pi, \mu, \Sigma)}{\partial \Sigma_j} = 0$ и $\cfrac{L(\pi, \mu, \Sigma)}{\partial \pi_j} = 0$. Нека означим решенията на тази система съответно с $\mu_j^{new}, \Sigma_j^{new}, \pi_j^{new}$. В \autoref{appendix:em} е показано, че 
\begin{flalign*}
    & \mu_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}x_i}{\sum\limits_{i=1}^N \gamma_{ij}} & \\
    & \Sigma_j^{new} = \begin{cases}
        \cfrac{\sum\limits_{i=1}^N \gamma_{ij} (x_{it} - \mu_{js})^2}{\sum\limits_{i=1}^N \gamma_{ij}}, & t=s \\
        0, & \text{иначе}
    \end{cases} & \\
    & \pi_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}}{N}, &\text{където } \gamma_{ij} = \cfrac{\pi_j \mathcal{N}(x_i, \mu_j, \Sigma_j)}{\sum\limits_{k=1}^K \pi_k \mathcal{N}(x_i, \mu_k, \Sigma_k)} &
\end{flalign*}
Правдоподобието относно Гаусова смеска може да се максимизира с ,,Алгоритъмът за максимизиране на очакването''\footnote{Expectation Maximisation} \cite{bishop} по следния начин:
\begin{exampleenv}
    \begin{enumerate}
        \item Разбиваме $X=(x_1,...,x_n)$ на $K$ части и взимаме за първоначални стойности на $\mu_j, \Sigma_j$ съответно средното и вариацията на $j-$тата част, а $\pi_j := \cfrac{\#_j}{|X|}$, където $\#_j = $ броят на векторите в $j-$тия клъстер.
        \item Пресмятаме $\gamma_{ij} = \cfrac{\pi_j \mathcal{N}(x_i, \mu_j, \Sigma_j)}{\sum\limits_{k=1}^K \pi_k \mathcal{N}(x_i, \mu_k, \Sigma_k)}$ с текущите стойности на модела. $\gamma_{ij}$ казва каква ,,тежест'' пада върху $j-$тата Гаусиана при генерирането на $i$-тия вектор.
        \item Пресмятаме 
        \begin{flalign*}
            & \mu_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}x_i}{\sum\limits_{i=1}^N \gamma_{ij}} & \Sigma_j^{new} = \begin{cases}
                \cfrac{\sum\limits_{i=1}^N \gamma_{ij} (x_{it} - \mu_{js}^new)^2}{\sum\limits_{i=1}^N \gamma_{ij}}, & t=s \\
                0, & \text{иначе}
            \end{cases} & \\
            & \pi_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}}{N}&
        \end{flalign*}
        \item Пресмятаме правдоподобието $\log(p(X|(\pi^e, \mu^e, \Sigma^e)))$
        \item Ако разликата между предишното и новото правдоподобие е по-малка от $\varepsilon = 1.10^{-5}$, то приключваме изпълнението с изход $(\pi, \mu, \Sigma)$, иначе се връщаме на стъпка $2$ с новите стойности на модела.
    \end{enumerate}
\end{exampleenv}

Важен момент е избирането на първоначалните $K$ клъстера. Емпирично е установено, че избирането на произволно разбиване може да забави намирането на оптимален модел. Затова често като първа стъпка се прави клъстеризация с K-means и по-точно модификацията K-means++. Разликата между K-means и K-means++ е в инициализацията на първоначалните центрове на клъстерите, които в K-means се избират на произволен принцип. В модификацията произволно се избира само първият център, а всеки от останалите $K-1$ центъра се избира вероятностно, като колкото по-отдалечена е една точка от вече избраните центрове, толкова по-вероятно е да бъде избрана. В \cite{kmeans} е показано, че намереното чрез K-means++ решение очаквано е най-много с фактор логаритъм по-лошо от оптималното.

\end{document}
