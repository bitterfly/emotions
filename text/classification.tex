\documentclass[main.tex]{subfiles}

\begin{document}
\section{Класификация}
\label{chap:em}

След като сме избрали характеристичните вектори, които ще извличаме по подаден файл, трябва да можем да ги класифицираме по някакъв начин.
В \cite{survey} са разгледани и сравнени различни методи за класификация. Макар че невронните мрежи са по-често използвани в последните публикации в областта,
тук ще се подходи по ,,старомодния'' начин с Гаусови смески. Показаното за тях съотношение между ,,прецизност на разпознаване'' и ,,време за трениране'' е най-добро спред проучването.

Целим да намерим разпределение за всяка от търсените емоции. Всяко непрекъснато разпределение може да се приближи с произволна точност със линейна комбинация на достатъчно на брой гаусиани, където теглата се сумират до 1. Такава сума ще наричаме Гаусова смеска. 

Нека за всяка емоция $e$ сме приближили разпределението на векторите ѝ със смеска от $K$ на брой гаусиани. Нека означим тази смеска с $(\pi^e, \mu^e, \Sigma^e)$, където 

$\pi^e = \{\pi_k^e\}_{k=1}^K$
$\mu^e = \{\mu_k^e\}_{k=1}^K$
$\Sigma^e = \{\Sigma_k^e\}_{k=1}^K$

Тогава при подаден нов характеристичен вектор $x$, ще търсим смеската на коя емоция ще доведе до най-голямо правдоподобие - тоест параметрите на кой модел е най-вероятно да генерират наблюдението. При подадени $(\pi^e, \mu^e, \Sigma^e)$ за дадена емоция $e$ и характеристичен вектор $x$, правдоподобието се пресмята с формулата:

$p(x) = \sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x\mid \mu_k^e, \Sigma_k^e)$,

където $\mu_i^e \in \mathbb{R}^{39}$, а $\sum\limits_1^K \pi_k^e = 1; 0\leq \pi_i^e\leq 1$

Попринцип ковариационната матрица $\Sigma_i^e \in (\mathbb{R}^{39 \times 39})$, но тъй като приемаме, че отделните MFCC коефициенти са независими, то значещ ще е само главният ѝ диагонал, тоест можем да приемем, че $\Sigma_i^e \in \mathbb{R}^{39}$.

Алгоритъмът за получаване на въпросните $\pi, \mu, \Sigma$ е следният.

Нека $X=(x_1,...x_n)$ са всички характеристични вектори с етикет $e$. Искаме правдоподобието на тези вектори, спрямо $(\pi^e, \mu^e, \Sigma^e)$ да е възможно най-голямо. Тоест искаме да оптимизираме

$p(X|(\pi^e, \mu^e, \Sigma^e)) = \prod\limits_{i=1}^{n} \sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x_i\mid \mu_k^e, \Sigma_k^e)$,

За по-голямо удобство, нека разглеждаме $\log(p(X|(\pi^e, \mu^e, \Sigma^e)))$. Тоест имаме
$\log(p(X|(\pi^e, \mu^e, \Sigma^e))) = log(\prod\limits_{i=1}^{n} \sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x_i\mid \mu_k^e, \Sigma_k^e)) = \sum\limits_{i=1}^{n} \log(\sum\limits_{k=1}^{K} \pi_k^e \mathcal{N}(x_i\mid \mu_k^e, \Sigma_k^e)))$, 

Нека сме фиксирали някаква емоция. Оптимизационната задача трябва да отчита ограниченията за $\pi$, затова има вида:

$L(\pi, \mu, \Sigma) = \sum\limits_{i=1}^{n} \log(\sum\limits_{k=1}^{K} \pi_k \mathcal{N}(x_i\mid \mu_k, \Sigma_k))) + \lambda(\sum\limits_{k=1}^K \pi_k - 1)$

За да максимизираме очакването, търсим решения съответно на \\$\cfrac{\partial L(\pi, \mu, \Sigma)}{\partial \mu_j}, \cfrac{\partial L(\pi, \mu, \Sigma)}{\partial \Sigma_j}$ и $\cfrac{L(\pi, \mu, \Sigma)}{\partial \pi_j}$, нека ги кръстим съответно $\mu_j^{new}, \Sigma_j^{new}, \pi_j^{new}$. В \autoref{appendix:em} е показано, че 
\begin{flalign*}
    & \mu_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}x_i}{\sum\limits_{i=1}^N \gamma_{ij}} & \\
    & \Sigma_j^{new} = \begin{cases}
        \cfrac{\sum\limits_{i=1}^N \gamma_{ij} (x_{it} - \mu_{js})^2}{\sum\limits_{i=1}^N \gamma_{ij}}, & t==s \\
        0, & \text{иначе}
    \end{cases} & \\
    & \pi_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}}{N}, &\text{където } \gamma_{ij} = \cfrac{\pi_j \mathcal{N}(x_i, \mu_j, \Sigma_j)}{\sum\limits_{k=1}^K \pi_k \mathcal{N}(x_i, \mu_k, \Sigma_k)} &
\end{flalign*}
Алгоритъмът за максимизиране на очакването е следният:
\begin{exampleenv}
    \begin{enumerate}
        \item Разделяме $X=(x_1,...,x_n)$ на $K$ клъстера и смятаме първоначалните $\mu_j, \Sigma_k$, а $\pi_j = \cfrac{\#_j}{|X|}$, където $\#_j = $ броят на векторите в $j-$тия клъстер 
        \item Пресмятаме $\gamma_{ij} = \cfrac{\pi_j \mathcal{N}(x_i, \mu_j, \Sigma_j)}{\sum\limits_{k=1}^K \pi_k \mathcal{N}(x_i, \mu_k, \Sigma_k)}$ с текущите стойности на модела. $\gamma_{ij}$ казва каква ,,тежест'' пада върху $j-$тата Гаусиана
        \item Пресмятаме 
        \begin{flalign*}
            & \mu_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}x_i}{\sum\limits_{i=1}^N \gamma_{ij}} & \Sigma_j^{new} = \begin{cases}
                \cfrac{\sum\limits_{i=1}^N \gamma_{ij} (x_{it} - \mu_{js}^new)^2}{\sum\limits_{i=1}^N \gamma_{ij}}, & t==s \\
                0, & \text{иначе}
            \end{cases} & \\
            & \pi_j^{new} = \cfrac{\sum\limits_{i=1}^N \gamma_{ij}}{N}&
        \end{flalign*}
        \item Пресмятаме правдоподобието $\log(p(X|(\pi^e, \mu^e, \Sigma^e)))$
        \item Ако условието за терминиране е изпълнено, то приключваме изпълнението с изход $(\pi, \mu, \Sigma)$, иначе се връщаме на $2.$
    \end{enumerate}
\end{exampleenv}
\end{document}
