\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Приложение за Максимизиране на ентропията}
\label{appendix:max_ent}

Нека имаме входни данни $\mathcal{D} = (x_1, y_1),\ldots, (x_n, y_n)$, където $x_i \in X$, а $y_i \in Y$, $X$ и $Y$ - изброими.

Търсим това разпределение $p$, което приближава разпределението, генерирало данните в $\mathcal{D}$, и което се държи равномерно иначе.

Тоест търсеното разпределение $p$, трябва да изпълнява:
\begin{flalign*}
	& p(x, y) = \tilde{p}(x) p(y|x), &&
\end{flalign*}

Тук с $\tilde{p}$ означаваме емпиричното разпределение, дефинирано като:

$\forall(x, y) \in X\times Y: \tilde{p}(x, y) = \cfrac{\#(x, y)}{n}$,  където
$\#(x, y) :=$ брой срещания на $(x, y)$ в $\mathcal{D}$.

С други думи:
\begin{flalign*}
	p(x) = \sum\limits_{y\in Y} p(x, y) & = \sum\limits_{y \in Y} \tilde{p}(x) p(y|x)&&\\
	&= \tilde{p}(x)\sum\limits_{y\in Y} p(y|x)&&\\& = \tilde{p}(x)
\end{flalign*}

и искаме да максимизира ентропията:
\begin{flalign*}
	H_p(X, Y) & = - \sum\limits_{(x, y) \in X\times Y} p(x, y) \log(p(x, y)) &&
\end{flalign*}

Тъй като: 
\begin{flalign*}
	& \sum\limits_{x\in Proj_1(\mathcal{D})} p(x) = \sum\limits_{x \in Proj_1(\mathcal{D})} \tilde{p}(x) = 1 = \sum\limits_{x \in Proj_1(\mathcal{D})} \tilde{p}(x) = \sum\limits_{x\in X} \tilde{p}(x) = \sum\limits_{x \in X} p(x), \text{то следва, че} &&\\
	& \sum\limits_{x\notin Proj_1(\mathcal{D})} p(x) = 0,&&
\end{flalign*}
тогава е достатъчно да искаме $p(x) = \tilde{p}(x)$ само за $x\in Proj_1(\mathcal{D})$

Нека имаме още множество  от характеристични функции $\mathcal{H}, |\mathcal{H}| = K$, които са от вида $h_i:X\times Y \rightarrow [0, 1]$.

Ако с $E(q, h)$ означим очакването на $h$, спрямо разпределение $q$, тоест:
\begin{flalign*}
	& E(q, h) = \sum\limits_{(x, y) \in X\times Y} q(x, y)h(x, y) &&
\end{flalign*}

То искаме за търсеното $p$ да е изпълнено:
\begin{flalign*}
	& E(p, h) = E(\tilde{p}, h), \forall h \in \mathcal{H} &&
\end{flalign*}

Ако дефинираме допълнително функции $h_{x_0}(x, y) = \begin{cases}
		1 & x = x_0\\
		0 & \text{иначе}
	\end{cases}$, то можем да изразим $p(x) = \tilde{p}(x)$ по следния начин:
\begin{flalign*}
	& p(x_0) = \sum\limits_{y\in Y} p(x_0, y) = \sum\limits_{(x, y) \in X \times Y} p(x, y) h_{x_0}(x, y) = E(p, h_{x_0})&&
	\\
	& \forall x \in Proj_1(\mathcal{D}): E(p, h_{x}) = E(\tilde{p}, h_{x})
\end{flalign*}


Ако означим:
\begin{flalign*}
	& P = \{ p | \B{\forall x \in Proj_1(\mathcal{D}): E(p, h_x) = E(\tilde{p}, h_x)} \land \B{\forall h \in \mathcal{H}: E(p, h) = E(\tilde{p}, h)}  \}&&,
\end{flalign*}
тогава искаме да намерим
\begin{flalign*}
	\hat{p} & = argmax_{p \in P} H_p(X, Y) &&\\
	& = argmax_{p \in P}\B{  - \sum\limits_{(x, y) \in X\times Y} p(x, y) \log(p(x, y))}&&\\
	& = argmax_{p \in P}\B{ -\sum\limits_{(x, y) \in X\times Y} p(x, y)\log(\tilde{p}(x)p(y|x)) }&&\\
	& = argmax_{p \in P}\B{ -\sum\limits_{(x, y) \in X\times Y} p(x, y)log(\tilde{p}(x))-\sum\limits_{(x, y) \in X\times Y} p(x, y)\log(p(y|x)) }&&\\
	& = argmax_{p \in P}\B{ -\sum\limits_{(x, y) \in X\times Y} \tilde{p}(x) p(y | x) log(\tilde{p}(x)) + H_p(Y|X) }&&\\
	& = argmax_{p \in P}\B{ -\sum\limits_{x\in X} \tilde{p}(x) log(\tilde{p}(x))\sum\limits_{y\in Y}p(y|x) + H_p(Y|X) }&&\\
	& = argmax_{p \in P}\B{ -\sum\limits_{x\in X} \tilde{p}(x) log(\tilde{p}(x)) + H_p(Y|X) }&&\\
	&  -\sum\limits_{x\in X} \tilde{p}(x)log(\tilde{p}(x)) \text{ е константа спрямо } p \text{, следователно:} &&\\
	& = argmax_{p\in P} H_p(Y|X)
\end{flalign*}

За да решим тази оптимизационна задача, ще ползваме множители на Лагранж. Тъй като имаме $K$ ограничения за всяка от характеристичните функции и трябва да отчетем, че търсим разпределение с определени свойства, задачата ще има вида:
\begin{flalign*}
	\Lambda(p, \tau, \lambda, \mu) = H_p(X, Y) &+ \sum\limits_{x\in Proj_1(\mathcal{D})} \tau_x\B{E(p, h_x) - E(\tilde{p}, h_x)} &&\\
	& + \sum\limits_{i=1}^{K} \lambda_i (E(p, h_i) - E(\tilde{p}, h_i )) +  \mu \sum\limits_{(x, y) \in X\times Y} p(x, y) -1 &&
\end{flalign*}

Нека фиксираме едно $x_0 \in X, y_0 \in Y$.
\begin{flalign*}
	\cfrac{\partial\B{\Lambda(p, \tau, \lambda, \mu)}}{\partial p(x_0, y_0)} = \cfrac{\partial H_p(X, Y)}{\partial p(x_0, y_0)} & +  \cfrac{\partial\B{\sum\limits_{x\in Proj_1(\mathcal{D})} \tau_x \B{E(p, h_x) - E(\tilde{p}, h_x)}}}{\partial p(x_0, y_0)} &&\\
	& + \cfrac{\partial\B{\sum\limits_{i=1}^K\lambda_i (E(p, h_i) - E(\tilde{p}, h_i ))}}{\partial p(x_0, y_0)} + \cfrac{\partial\B{\mu \sum\limits_{(x, y) \in X \times Y} p(x, y) -1}}{\partial p(x_0, y_0)}&&\\
\end{flalign*}
\begin{flalign*}
	& = \cfrac{\partial\B{-\sum\limits_{(x, y) \in X\times Y} p(x, y)\log(p(x, y))}}{\partial p(x_0, y_0)} +  \cfrac{\partial\B{\sum\limits_{x\in Proj_1(\mathcal{D})} \tau_x\Q{\sum\limits_{(x', y)\in X\times Y} p(x', y)h_x(x', y) }}}{\partial p(x_0, y_0)} &&\\
	&\quad + \cfrac{\partial\B{\sum\limits_{i=1}^K \lambda_i\Q{\sum\limits_{(x, y)\in X\times Y} p(x, y) h_i(x, y)}}}{\partial p(x_0, y_0)} + \mu && \\
	\\
	& = -log(p(x_0, y_0)) - 1 + \sum\limits_{x\in Proj_1(\mathcal{D})}\tau_{x}h_{x}(x_0, y_0) + \sum\limits_{i=1}^K \lambda_i h_i(x_0, y_0) + \mu\\
	\\
	& = -log(p(x_0, y_0)) - 1  + \sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0) + \mu,\\
\end{flalign*}
където $K' = K + |Proj_1(\mathcal{D})|$ и ако номерираме $x$-овете в $Proj_1(\mathcal{D})$ от $K+1$ до $K'$, то $\forall j = K+1,\ldots, K': \lambda_j = \tau_{x_j}$, а $h_j = h_{x_j}$

Искаме да нулираме производната:

\begin{flalign}
	\label{appendix:max_ent:00}
	\nonumber & -log(p(x_0, y_0)) - 1 + \sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0) + \mu = 0 \longleftrightarrow && \\
	\nonumber & log(p(x_0, y_0)) = \sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0) + \mu  - 1\longleftrightarrow && \\
	& p(x_0, y_0) = exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0)} exp\B{\mu -1} &&
\end{flalign}

Производната по $\mu$ ни дава:
\begin{flalign*}
	&\sum\limits_{(x, y)\in X \times Y} p(x, y) = 1 \longleftrightarrow && \\
	& \sum\limits_{(x, y)\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x, y) + \mu - 1} = 1 \longleftrightarrow && \\
	& exp\B{\mu - 1}\sum\limits_{(x, y)\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x, y)} = 1 &&\\
	& \longleftrightarrow &&\\
	& exp\B{\mu - 1} = \cfrac{1}{\sum\limits_{(x, y)\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x, y)}} &&\\
	& \text{Заместваме в \autoref{appendix:max_ent:00}:} &&\\
	& p(x_0, y_0) = \cfrac{\exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0)}}{\sum\limits_{(x, y)\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x, y)}} &&
\end{flalign*}

Тъй като $p(x, y) = p(x) p(y|x)$, можем да получим, че:
\begin{flalign*}
	p(y_0|x_0) & = \cfrac{p(x_0, y_0)}{p(x_0)} = \cfrac{p(x_0, y_0)}{\sum\limits_{y\in Y}p(x_0, y)} &&\\
	& = \cfrac{\exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0)}}{\sum\limits_{(x', y')\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x', y')}} \div \sum\limits_{y \in Y}\B{\cfrac{\exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y)}}{\sum\limits_{(x', y')\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x', y')}} } &&\\
	& = \cfrac{\exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0)}}{\cancel{\sum\limits_{(x', y')\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x', y')}}} \div \cfrac{\sum\limits_{y \in Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y)}}{\cancel{\sum\limits_{(x', y')\in X\times Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x', y')}}} &&\\
	\\
	& = \cfrac{\exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y_0)}}{\sum\limits_{y\in Y} \exp\B{\sum\limits_{i=1}^{K'} \lambda_i h_i(x_0, y)}} = \cfrac{\exp\B{\sum\limits_{i=1}^{K} \lambda_i h_i(x_0, y_0)} \cancel{\exp\B{\sum\limits_{i=K+1}^{K'} \lambda_i h_i(x_0, y_0)}} }{\cancel{\exp\B{\sum\limits_{i=K+1}^{K'} \lambda_i h_i(x_0, y)}} \sum\limits_{y\in Y} \exp\B{\sum\limits_{i=1}^{K} \lambda_i h_i(x_0, y)}} &&\\\\
	& = \cfrac{\exp\B{\sum\limits_{i=1}^{K} \lambda_i h_i(x_0, y_0)}}{\sum\limits_{y\in Y} \exp\B{\sum\limits_{i=1}^{K} \lambda_i h_i(x_0, y)}}, &&
\end{flalign*}
тъй като за $i=K+1,\ldots,K', h_i(x, y)$ не зависят от избора на $y$.

Следователно вида на търсеното $\hat{p}$ е $\hat{p}(x, y) = \pi\prod\limits_{i=1}^{K'} e^{\lambda_i h_i(x, y)}$, като $\pi$ е нормализиращата константа. Ще покажем че $\hat{p}$, което минимизира ентропията, също минимизира и условното правдоподобие.

Нека с $Q$ означим всички разпределения с желания вид. Имаме, че $Q = \{p \ | \  p(x, y) = \pi\prod\limits_{i = 1}^{K'} e^{\lambda_i h_i(x, y)}\}$. За да намерим оптималното разпределение, ще ни е нужно да дефинираме разстояние между разпределения - ``Разстояние'' на Кулбек-Лайблър:
\begin{flalign*}
	D(p, q) = \sum\limits_{(x, y) \in X\times Y} p(x, y) \log\B{\cfrac{p(x, y)}{q(x, y)}}
\end{flalign*}

``Разстоянието'' на Кулбек-Лайблър всъщност не е разстояние в математическия смисъл (в смисъла на метрика), тъй като не е симетрична функция, но често се използва за разстояние между разпределения, тъй като има този интуитивен смисъл. Затова ще продължим да го наричаме разстояние, пропускайки кавичките.

С това сме готови да покажем следните твърдения:

\begin{lemma}
	\label{appendix:max_ent:01}
	За всеки две разпределения $p$ и $q$, $D(p, q) \geq 0$, като $D(p, q) = 0 \iff p = q$

	\begin{proof}
		Тъй като $p$ е разпределение и е изпълнено, че $\sum\limits_{(x, y) \in X\times Y} p(x, y) = 1$, можем да приложим неравенството на Йенсен:
		\begin{flalign*}
			& \sum\limits_{i = 1}^{\infty} p(x_i, y_i) f(z_i) \leq f\B{\sum\limits_{i = 1}^{\infty} p(x_i, y_i) z_i}, \forall i: z_i \in \mathbb{R}, &&
		\end{flalign*}
		където $f$ е вдлъбната. Ако за $f$ е изпълнено, че $f'' < 0$, то равенство се достига, когато $z_i$ е константа.

		\begin{flalign*}
			-D(p, q) & = -\sum\limits_{(x, y) \in X\times Y} p(x, y) \log\B{\cfrac{p(x, y)}{q(x, y)}}&&\\
			& = \sum\limits_{(x, y) \in X\times Y} p(x, y) \log\B{\cfrac{q(x, y)}{p(x, y)}}&&\\
			& \leq \log\B{\sum\limits_{(x, y) \in X\times Y} \cancel{p(x, y)}\cfrac{q(x, y)}{\cancel{p(x, y)}} }&&\\
			& = \log\B{\sum\limits_{(x, y) \in X\times Y} q(x, y)} = 0 &&\\
			& \longleftrightarrow D(p, q) \geq 0
		\end{flalign*}

		Тъй като втората производна на логаритъма е винаги отрицателна, равенство при неравенството на Йенсен се достига, когато $\cfrac{q(x, y)}{p(x, y)}$ е константа, тоест: 
		\begin{flalign*}
		& q(x, y) = Cp(x, y) &&\\
		& \sum\limits_{(x, y) \in \mathcal{D}} q(x, y) = \sum\limits_{(x, y) \in \mathcal{D}} C p(x, y) &&\\
		& \longleftrightarrow C = 1 &&\\
		& \longleftrightarrow p(x, y) = q(x, y) \forall (x, y) \in X\times Y &&
		\end{flalign*}
	\end{proof}
\end{lemma}

\begin{lemma}
	\label{appendix:max_ent:02}
	За всеки $p_1, p_2 \in P, q\in Q$ е изпълнено:
	
	$\sum\limits_{(x, y) \in X\times Y} p_1(x, y) \log(q(x, y)) = \sum\limits_{(x, y) \in X\times Y} p_2(x, y) \log(q(x, y))$

	\begin{proof}
		\begin{flalign*}
			&\sum\limits_{(x, y) \in X\times Y} p_1(x, y) \log(q(x, y))&&\\
			& = \sum\limits_{(x, y) \in X\times Y} p_1(x, y)\log\B{\pi\prod\limits_{i = 1}^{K'} e^{\lambda_i h_i(x, y)}} &&\\
			& = \sum\limits_{(x, y) \in X\times Y} p_1(x, y)\B{\log(\pi) + \log\B{ \prod\limits_{i = 1}^{K'} e^{\lambda_i h_i(x, y)}}} &&\\
			& = \sum\limits_{(x, y) \in X\times Y} p_1(x, y)\B{\log(\pi) + \sum\limits_{i = 1}^{K'} \log\B{ e^{\lambda_i h_i(x, y)}}} &&\\
			& = \sum\limits_{(x, y) \in X\times Y} p_1(x, y)\B{\log(\pi) + \sum\limits_{i = 1}^{K'} \lambda_i h_i(x, y)} &&\\
			& = \sum\limits_{(x, y) \in X\times Y} p_1(x, y)\log(\pi) + \sum\limits_{(x, y) \in X\times Y} p_1(x, y)\sum\limits_{i = 1}^{K'} \lambda_i h_i(x, y) &&\\
			& = \log(\pi)\sum\limits_{(x, y) \in X\times Y} p_1(x, y) + \sum\limits_{(x, y) \in X\times Y} \sum\limits_{i = 1}^{K'} p_1(x, y) \lambda_i h_i(x, y) &&\\
			& = \log(\pi). 1 + \sum\limits_{(x, y) \in X\times Y} \sum\limits_{i = 1}^{K'} p_1(x, y) \lambda_i h_i(x, y) &&\\
			& = \log(\pi). 1 +  \sum\limits_{i = 1}^{K'} \lambda_i \sum\limits_{(x, y) \in X\times Y} p_1(x, y)  h_i(x, y) &&\\
			&\text{Тъй като } p_2 \in P \text{ и } E(p_1, h) = E(\tilde{p}, h_i) = E(p_2, h_i) \forall i = 1,\ldots, K': &&\\
			& = \log(\pi). 1 +  \sum\limits_{i = 1}^{K'} \lambda_i \sum\limits_{(x, y) \in X\times Y} p_2(x, y)  h_i(x, y) &&\\
			& \text{Използваме и че } \sum\limits_{(x, y) \in X\times Y} p_2(x, y) = 1 &&\\
			& = \log(\pi)\sum\limits_{(x, y) \in X\times Y} p_2(x, y) +  \sum\limits_{i = 1}^{K'} \lambda_i \sum\limits_{(x, y) \in X\times Y} p_2(x, y)  h_i(x, y) &&\\
			&= \sum\limits_{(x, y) \in X\times Y} p_2(x, y) \log(q(x, y))&&\\
		\end{flalign*}
	\end{proof}
\end{lemma}

\begin{lemma}
	\label{appendix:max_ent:03}
	Ако $p \in P, q \in Q, r \in P\cap Q$, то $D(p, q) = D(p, r) + D(r, q)$

	\begin{proof}
		\begin{flalign*}
			& D(p, r) + D(r, q) =&&\\
			& = \sum\limits_{(x, y)\in X\times Y}p(x, y)\log\B{\cfrac{p(x, y)}{r(x, y)}} + \sum\limits_{(x, y)\in X\times Y}r(x, y)\log\B{\cfrac{r(x, y)}{q(x, y)}}&&\\
			& = \sum\limits_{(x, y)\in X\times Y}p(x, y)\log(p(x, y)) - \sum\limits_{(x, y)\in X\times Y}p(x, y) \log(r(x, y)) + &&\\
			& \quad\sum\limits_{(x, y)\in X\times Y}r(x, y)\log(r(x, y)) - \sum\limits_{(x, y)\in X\times Y}r(x, y) \log(q(x, y))&&\\
			& \text{по \autoref{appendix:max_ent:02} за } p,r \in P \text{ и } r\in Q \\
			& = \sum\limits_{(x, y)\in X\times Y}p(x, y)\log(p(x, y)) - \cancel{\sum\limits_{(x, y)\in X\times Y}p(x, y) \log(r(x, y))} + &&\\
			& \cancel{\quad\sum\limits_{(x, y)\in X\times Y}{\color{red}p(x, y)}\log(r(x, y))} - \sum\limits_{(x, y)\in X\times Y}r(x, y) \log(q(x, y))&&\\
			& &&\\
			& = \sum\limits_{(x, y)\in X\times Y}p(x, y)\log(p(x, y)) - \sum\limits_{(x, y)\in X\times Y}r(x, y) \log(q(x, y)) &&\\
			& \text{по \autoref{appendix:max_ent:02} за } p,r \in P \text{ и } q\in Q \\
			& = \sum\limits_{(x, y)\in X\times Y}p(x, y)\log(p(x, y)) - \sum\limits_{(x, y)\in X\times Y}{\color{red} p(x, y)} \log(q(x, y)) &&\\
			& = \sum\limits_{(x, y)\in X\times Y}p(x, y) \log\B{\cfrac{p(x, y)}{q(x, y)}} = D(p, q) &&\\
		\end{flalign*}
	\end{proof}
\end{lemma}

\begin{lemma}
	\label{appendix:max_ent:04}
	Ако $r \in P\cap Q$, то $r$ е единствено и $r = \hat{p}$

	\begin{proof}

		Нека $r \in P\cap Q$. Условието $r = \hat{p}$ значи, че $r = argmax_{p \in P} H_p(X, Y)$, тоест ще покажем, че за всяко $p \in P: H_r(X, Y) \geq H_p(X, Y)$.

		Нека $u \in Q$, такова че $u(x, y) \neq 0, \forall (x, y) \in X\times Y$. Всъщност всяко разпределение $q$ от $Q$ е такова, защото $\sum\limits_{(x, y) \in X\times Y}e^{\bullet} > 0$, а $\pi \neq 0$, защото $\pi$ е константа и ако $\pi = 0$, тогава $\sum\limits_{(x, y) \in X\times Y} q(x, y) = 0$ и не изпълнява условието за разпределение. 

		Нека фиксираме произволно $p \in P$. Тогава от \autoref{appendix:max_ent:03} следва, че
		\begin{flalign*}
			& D(p, u)  = D(p, r) + D(r, u) &&\\
			& D(p, u) \quad \coolgeq{\autoref{appendix:max_ent:01}} \quad D(r, u) &&\\
			& \sum\limits_{(x, y) \in X\times Y} p(x, y) \log\B{\cfrac{p(x, y)}{u(x, y)}} \geq \sum\limits_{(x, y) \in X\times Y} r(x, y) \log\B{\cfrac{r(x, y)}{u(x, y)}} && \\
			& -H_p(X, Y) - \sum\limits_{(x, y) \in X\times Y} p(x, y) \log(u(x, y)) \geq -H_r(X, Y) - \sum\limits_{(x, y) \in X\times Y} r(x, y) \log(u(x, y)) &&\\
			& \text{по \autoref{appendix:max_ent:02} за } p,r \in P \text{ и } u\in Q \text{ следва}:\\
			& -H_p(X, Y) - \cancel{\sum\limits_{(x, y) \in X\times Y} p(x, y) \log(u(x, y))} \geq -H_r(X, Y) - \cancel{\sum\limits_{(x, y) \in X\times Y} {\color{red} p(x, y)} \log(u(x, y))} &&\\
			& H_r(X, Y) \geq H_p(X, Y) &&
		\end{flalign*}
		Следователно $r = argmax_{p \in P} H_p(X, Y)$

		Сега нека видим защо $r$ е единствено.
		Нека $r' = argmax_{p \in P} H_p(X, Y)$. Тогава:
		\begin{flalign*}
			& H_{r'}(X, Y) = H_r(X, Y) \longleftrightarrow D(r, u) = D(r', u) && \\
			& \text{ но } D(r, u) = D(r, r') + D(r', u) \text{ по }\autoref{appendix:max_ent:03} && \\
			& \Longrightarrow \quad D(r, r') = 0 &&\\
			& \coolra{\autoref{appendix:max_ent:01}} \quad r = r' &&\\
		\end{flalign*}
	\end{proof}
\end{lemma}

Дефинираме функцията $L(p)$:
\begin{flalign*}
	L(p) & = \sum\limits_{(x, y) \in X\times Y} \tilde{p}(x, y) \log(p(x, y)) &&\\
\end{flalign*}


\begin{lemma}
	\label{appendix:max_ent:05}
	Ако $r \in P\cap Q$, то $r$ е единствено и $r = argmax_{q \in Q} L(q)$

	\begin{proof}
		Искаме да покажем, че за всяко $q \in Q: L(r) \geq L(q)$.

		Нека фиксираме едно $q \in Q$, а $\tilde{p}$ е емпиричното разпределение и следователно $\tilde{p} \in P$ по дефиниция.

		Тогава от \autoref{appendix:max_ent:03} следва, че:
		\begin{flalign*}
			& D(\tilde{p}, q)  = D(\tilde{p}, r) + D(r, q) &&\\
			& D(\tilde{p}, q) \quad \coolgeq{\autoref{appendix:max_ent:01}} \quad D(\tilde{p}, r) &&\\
			& \sum\limits_{(x, y) \in X\times Y} \tilde{p}(x, y) log\B{\cfrac{\tilde{p}(x, y)}{q(x,y)}} \geq \sum\limits_{(x, y) \in X\times Y} \tilde{p}(x, y) log\B{\cfrac{\tilde{p}(x, y)}{r(x, y)}} &&\\
			& -\cancel{H_{\tilde{p}}(X, Y)} - L(q) \geq -\cancel{H_{\tilde{p}}(X, Y)} - L(r) && \\
			& \longleftrightarrow L(r) \geq L(q)
		\end{flalign*}

		Сега нека $r' = argmax_{q \in Q} L(q)$, тоест $L(r) = L(r') \Longrightarrow D(\tilde{p}, r) = D(\tilde{p}, r')$.

		Но по \autoref{appendix:max_ent:03}, $D(\tilde{p}, r') = D(\tilde{p}, r) + D(r, r') \Longrightarrow D(r, r') = 0 \coollra{\autoref{appendix:max_ent:01}} r' = r$, следователно $r$ е единствено.
	\end{proof}
\end{lemma}

Сега, нека дефинираме условно правдоподобие на разпределение $p$ при дадено множество $\mathcal{D}$ като:
\begin{flalign*}
	& \widehat{L}_{\mathcal{D}}(Y|X) = \prod\limits_{(x, y) \in X\times Y} p(y|x)^{\#(x, y)} &&
\end{flalign*}
Тъй като логаритъмът е вдлъбната и монотонно растяща функция, често се разглежда за удобство:
\begin{flalign*}
	& log\B{\widehat{L}_{\mathcal{D}}(Y | X)} = \sum\limits_{(x, y) \in X\times Y} \#(x, y) log(p(y | x)) &&
\end{flalign*}

Тъй като $\hat{p} \in P \cap Q$, по горното твърдение имаме:
\begin{flalign*}
	\hat{p} & = argmax_{p\in Q} L(p) &&\\
	& = argmax_{p\in Q}\B{\sum\limits_{(x, y) \in X\times Y} \tilde{p}(x, y) log(p(x, y))} &&\\
	& = argmax_{p\in Q}\B{\sum\limits_{(x, y) \in X\times Y} \tilde{p}(x, y) log(\tilde{p}(x)p(y|x))} &&\\
	& = argmax_{p\in Q} \B{\sum\limits_{(x, y) \in X\times Y} \tilde{p}(x, y)log(\tilde{p}(x)) + \sum\limits_{(x, y)\in X\times Y} \tilde{p}(x, y)log(p(y | x))}&&\\
	&  = argmax_{p\in Q} \B{\sum\limits_{(x, y)\in X\times Y} \tilde{p}(x, y)log(p(y | x))}&&\\
	&  = argmax_{p\in Q} \B{0 + \sum\limits_{(x, y)\in \mathcal{D}} \cfrac{\#(x, y)}{n} log(p(y | x))}&&\\
	&  = argmax_{p\in Q} \B{\sum\limits_{(x, y)\in \mathcal{D}} \#(x, y) log(p(y | x))}&&\\
	& = argmax_{p\in Q} \B{log\B{\widehat{L}_{\mathcal{D}}(Y|X) }}
\end{flalign*}


От \autoref{appendix:max_ent:04} и \autoref{appendix:max_ent:05}, че ако вземем разпределение от сечението на $P$ и $Q$, то е единствено и е равно на $\hat{p} = argmax_{p \in P} H_p(X, Y) =  argmax_{p \in P} H_p(Y|X) = argmax_{q \in Q} L(q) = argmax_{q\in Q} log\B{\widehat{L}_{\mathcal{D}}(Y|X) }$
\end{document}
