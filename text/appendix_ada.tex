\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Приложение за AdaBoost}
\label{appendix:ada}
Ще разгледаме алгоритъма AdaBoost в дискретния случай, както оригинално е представен в фдсдф.
Задачата, която имаме е следната:
Имаме тренировъчни данни $M := (x_1, y_1), (x_2, y_2), \cdots (x_n, y_n), x_i \in \mathbb{R}^n, y_i \in \{-1, 1\}, i = 1 \cdots n$. Имаме множество $\mathcal{H}$ от функции от вида $h_i : \mathbb{R}^n \rightarrow \{-1, 1\}$ и е дадена константа $T$. Търсим $T$ функции от $\mathcal{H}$, които да съчетаем до получаването на нова функция, която е линейна комбинация на избраните $T$:
\[
	H(x) = \sum\limits_{i=1}^T \alpha_i h_i(x)
\]
AdaBoost алгоритъмът избира последователно функции $h_i$ от $\mathcal{H}$ и избира за всяка тегло $\alpha_i$. С $H_t(x)$ ще означаваме линейната комбинация, получена от първи $t$ избрани класификатора и имаме, че:
\begin{flalign}
	\label{appendix:ada:01}
	\nonumber H_t(x) & = \sum\limits_{i=1}^t \alpha_i h_i(x)\\
	\nonumber & = H_{t-1}(x) + \alpha_t h_t(x)\\
	\nonumber \\
	H_0(x) & = 0 \quad \forall x \in \mathbb{R}^n
\end{flalign}

Идеята е $t$-тата избрана хипотеза да поправи грешките, които предните $t-1$ хипотези правят върху тренировъчното множество.

На всяка итерация $t$ на лагоритъма, дефинираме разпределение върху тренировъчните данни, което ще означаваме с $\mathcal{D}_t$, където $\mathcal{D}_t(i)$ дава вероятност на $i$-тия пример. Идеята е да може да се даде по-голяма вероятност на тези примери, върху които предните $t-1$ избрани функции бъркат и да се избере функция, която се представя добре върху така претеглените данни. В началото на алгоритъма за $\mathcal{D}_1$ избираме равномерно разпределение, тоест:
\[\mathcal{D}_1(i) = \cfrac{1}{n}, i = 1\cdots n\]

Алгоритъмът е следният:

\begin{exampleenv}
	\begin{alg}
		\item $D_1(i) = \cfrac{1}{n}, i = 1\cdots n$
		\item $\mathcal{H} = \emptyset$
		\item За всяко $t$ от 1 до $T$ се прави следното:
		\begin{alg}
			\item $h_t = argmin_{h\in \mathcal{H}} P_{i\sim D_t}(h(x_i) \neq y_i)$
			\item $\varepsilon_t = P_{i\sim D_t}(h_t(x_i) \neq y_i)$
			\item $\alpha_t = \cfrac{1}{2}\ln\B{\cfrac{1 - \varepsilon_t}{\varepsilon_t}}$
			\item $\mathcal{H}_t = \mathcal{H}_{t-1} \cup \{(h_t, \alpha_t)\}$
			\item За всяко i от 1 до n:
			\begin{alg}
				\item $\mathcal{D}_{t+1}(i) = \cfrac{\mathcal{D}_t(i) e^{-\alpha_t y_i h_t(x_i)}}{\sum\limits_{j=1}^n \mathcal{D}_t(j) e^{-\alpha_t y_j h_t(x_j)}}$
			\end{alg} 
		\end{alg}
		\item $\mathcal{H} = \mathcal{H}_T$
		\item Връщаме $\mathcal{H}$
	\end{alg}
\end{exampleenv}

Вижда се, че изборът на разпределението $\mathcal{D}(t+1)$ е такова, че дава висока вероятност на примерите, върху които предният класификатор $h_t$ греши и обратното. Тоест, ако $h_t(x_i) = y_i, e^{-\alpha_t y_i h_i(x_i)} = e^{-\alpha_t}$, тъй като $y_i$ и $h_t(x_i)$ са с еднакъв знак, а при $h_t(x_i) \neq y_i, e^{-\alpha_t y_i h_i(x_i)} = e^{\alpha_i}$. Тъй като $\alpha_t \geq 0$, това означава, че грешката на класификаторите, от които избираме, не трябва да надхвърля $\cfrac{1}{2}$ за бинарни класификатори. При модификацията за няколко класа, трябва да е изпълнено, че грешката е не повече от $\cfrac{1}{K}$, където $K$ е броят на класовете.

Целта на AdaBoost алгоритъмът е да се намери такова $\mathcal{H}$, което минимизира експоненциалната грешка, която се пресмята по формулата:
\[
	\mathcal{l}(h, x, y) = e^{-yh(x)}
\]

За да видим, че горният алгоритъм намира такова $\mathcal{H}$, ще разгледаме следните твърдения. 

Нека $w_{t,i} = e^{-y_i\mathcal{H}_{t-1}(x_i)}$

\begin{lemma}
	$\mathcal{D}_t = \cfrac{w_{t, i}}{\sum\limits_{j=1}^n w_{t, j}}$ 
\end{lemma}

\begin{proof}
	Ще го докажем по индукция.
	\begin{itemize}
		\item База $t = 1$
		\begin{flalign*}
			\cfrac{w_{1, i}}{\sum\limits_{j=1}^n w_{1, j}} & = \cfrac{e^{-y_i\mathcal{H}_{0}(x_i)}}{\sum\limits_{j=1}^n e^{-y_j\mathcal{H}_{0}(x_j)}}&&\\
			& \stackrel{\mathclap{\normalfont\mbox{\tiny{\ref{appendix:ada:01}}}}}{=\joinrel=} \cfrac{1}{n} &&\\
			& \stackrel{\mathclap{\normalfont\mbox{\tiny{ред 1.}}}}{=\joinrel=} \mathcal{D}_1(i) \text{ за всяко i} &&
		\end{flalign*}
		\item $\mathcal{D}_t = \cfrac{w_{t, i}}{\sum\limits_{j=1}^n w_{t, j}} \Rightarrow \mathcal{D}_{t+1} = \cfrac{w_{t+1, i}}{\sum\limits_{j=1}^n w_{t+1, j}}$
		\begin{flalign*}
			\mathcal{D}_{t+1} & = \cfrac{w_{t+1, i}}{\sum\limits_{j=1}^n w_{t+1, j}} && \\
			& \stackrel{\mathclap{\normalfont\mbox{\tiny{ред 3.5.1.}}}}{=\joinrel=} \cfrac{\mathcal{D}_t e^{-\alpha_t y_i h_t(x_i)}}{\sum\limits_{j=1}^n \mathcal{D}_t e^{-\alpha_t y_j h_t(x_j)} } && \\
			& \stackrel{\mathclap{\normalfont\mbox{\tiny{ИХ}}}}{=\joinrel=} \cfrac{\frac{1}{1} e^{-\alpha_t y_i h_t(x_i)}}{\sum\limits_{j=1}^n \frac{1}{1} e^{-\alpha_t y_j h_t(x_j)} } && \\
		\end{flalign*}

	\end{itemize}
\end{proof}

\begin{lemma}
	$P_{i\sim D_t}(h(x_i) \neq y_i) = \sum\limits_{i:h(x_i) \neq y_i} \cfrac{e^{y_i \mathcal{H}_{t-1}(x_i)} }{\sum\limits_{j=1}^n e^{y_j \mathcal{H}_{t-1}(x_j)}}$
\end{lemma}


\begin{lemma}
	Изборът на $h_t = argmin_{h\in \mathcal{H}} P_{i\sim D_t}(h(x_i) \neq y_i)$ от точка 3.2 на алгоритъма минимизира експоненциалната грешка на $\mathcal{H}_t$ върху тренировъчното множество, тоест:
	\[
		h_t = argmin_{h\in \mathcal{H}} \B{ \cfrac{1}{n}\sum\limits_{i=1}^n \mathcal{l}(\mathcal{H}_{t-1} + Ch, x_i, y_i)},
	\]
	където $C$ е произволна константа.
\end{lemma}

\begin{proof}
\begin{flalign*}
	h_t & = argmin_{h\in \mathcal{H}} \B{ \cfrac{1}{n}\sum\limits_{i=1}^n \mathcal{l}(\mathcal{H}_{t-1} + Ch, x_i, y_i)} &&\\
	& = argmin_{h\in \mathcal{H}} \B{ \cfrac{1}{n}\sum\limits_{i=1}^n e^{-y_i(\mathcal{H}_{t-1}(x_i) + Ch(x_i))}} &&\\
	& = argmin_{h\in \mathcal{H}} \B{ \cfrac{1}{n}\sum\limits_{i=1}^n e^{-y_i\mathcal{H}_{t-1}(x_i)} e^{-y_iCh(x_i)}} &&
\end{flalign*}
Полагаме  $w_{t,i} = e^{-y_i\mathcal{H}_{t-1}(x_i)}$ и махаме константата $\cfrac{1}{n}$, тъй като тя не влияе на минимизацията.

\begin{flalign*}
	& = argmin_{h\in \mathcal{H}} \B{\sum\limits_{i=1}^n w_{t, i} e^{-y_iCh(x_i)}} &&\\
\end{flalign*}

Сега можем да разделим сумата на две, в зависимост дали $h(x_i) = y_i$. Ако това е изпълнено, то $y_i h(x_i) = 1$ и е равно на -1 в противен случай, тоест:

\begin{flalign*}
	& = argmin_{h\in \mathcal{H}} \B{\sum\limits_{i: h(x_i) = y_i}^n w_{t, i} e^{-C} + \sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i} e^{C}} &&\\
	& = argmin_{h\in \mathcal{H}} \B{\Q{\sum\limits_{i=1}^n w_{t, i} e^{-C} - \sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i} e^{-C}}+ \sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i} e^{C}} &&\\
	& = argmin_{h\in \mathcal{H}} \B{\sum\limits_{i=1}^n w_{t, i} e^{-C} + \sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i} (e^{C} - e^{-C}) } &&\\
\end{flalign*}
$\sum\limits_{i=1}^n w_{t, i} e^{-C}$ е константа спрямо $h$, затова също не участва в минимизацията и тогава:

\begin{flalign*}
	& = argmin_{h\in \mathcal{H}} \B{\sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i} (e^{C} - e^{-C}) } &&\\
	& = argmin_{h\in \mathcal{H}} \B{ (e^{C} - e^{-C}) \sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i}} &&\\
	& = argmin_{h\in \mathcal{H}} \B{\sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i}}, &&\\
\end{flalign*}
тъй като и $ (e^{C} - e^{-C}) $ не зависи от $h$. Можем да умножим по константата $\cfrac{1}{\sum\limits_{j=1}^n w_{t, j}}$ и да получим:

\begin{flalign*}
	& = argmin_{h\in \mathcal{H}} \B{\cfrac{\sum\limits_{i: h(x_i) \neq y_i}^n w_{t, i}}{\sum\limits_{j=1}^n w_{t, j}} }, &&\\
	& = argmin_{h\in \mathcal{H}} \B{\sum\limits_{i: h(x_i) \neq y_i}^n \cfrac{w_{t, i}}{\sum\limits_{j=1}^n w_{t, j}} }, &&\\
	& = P_{i\sim D_t}(h(x_i) \neq y_i) &&
\end{flalign*}


\end{proof}
\hrulefill
\end{document}