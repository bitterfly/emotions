\documentclass[main.tex]{subfiles}

\begin{document}

В предните раздели описахме получаването на класификатори за сигнали от реч и ЕЕГ поотделно. Сега въпросът е дали можем да съчетаем по някакъв начин данните или класификаторите с цел да получим по-добра класификационна точност. Разгледаните подходи са два. Първият е наивен и съчетава самите характеристични вектори, а вторият - вече получените класификатори. 

\section{Съчетаване чрез конкатенация на характеристичните вектори}
\subsection{Описание}
Тук идеята е възможно най-проста - конкатенираме характеристичните вектори от речта и тези от ЕЕГ сигнала до получаване на нов вектор и тренираме класификатора, описан в \autoref{chap:em}. Единствената трудност е, че векторите за реч се взимат много по-често. За да има еднакъв брой вектори за двата сигнала, тези за речта се осредняват на всеки 200 ms.
\subsection{Резултати}
На таблица \autoref{tab:double:results:01} е показана средната класификационна точност за всяка една от емоциите за двата класификатора поотделно и накрая за този, получен при конкатенацията на векторите. Всяка от Гаусовите смески на класификатора на реч има 8 гаусиани, докато този за ЕЕГ и полученият при конкатенация имат по 3 гаусиани. От таблицата се вижда, че този метод не довежда до подобрение. Това вероятно се дължи на малкото количество данни, тъй като векторите, с които работим, са $39 + 76 = 115$ мерни. Това означава, че за да видим повече проявления на характеристиките, ще са нужни много повече данни. Освен набавянето на допълнително данни, може да се намали пространството чрез факторен анализ, за да се подобри обучението на класификатора.

\begin{table}[h]
    \begin{center}
    \begin{tabular}{|l|r r r|} 
        \hline
        Емоция & Само реч & Само ЕЕГ & Конкатенация \\ 
        \hline
        Гняв & 85.00\% & 80.00\% & 85.00\%\\ 
        Щастие & 75.00\% & 75.00\% & 50.00\%\\ 
        Неутрално & 7.50\% & 97.50\% & 97.50\%\\ 
        Тъга & 85.42\% & 87.50\% & 87.50\%\\ 
        \hline
        \hline
        Общо & \textbf{63.23\%} & \textbf{85.00\%} &  \textbf{80.00\%}\\
        \hline
    \end{tabular}
    \caption{Класификационна точност на класификатор за реч, класификатор за ЕЕГ и класификатор, получен при конкатенация на характеристичните вектори}
    \label{tab:double:results:01}
    \end{center}
\end{table}

\section{Съчетаване чрез максимизиране на ентропията}
\subsection{Описание}
Другият похват, който е приложен използва модел максимизиращ ентропията. Идеята е да намерим такова разпределение $p$ върху входните дани, което изпълнява дадените ограничения, но в същото време не прави допълнителни предположения, тоест максимизира ентропията.
Тоест имайки входни данни от вида $\mathcal{D} = (x_1, y_1)\ldots (x_n, y_n)$, където $x_i \in X$ са характеристични вектори с етикети $y_i \in Y$, където $Y = \{1,\ldots K\}$ представя множеството от търсените емоции, искаме да максимизираме ентропията:
\[
H(p) = - \sum\limits_{(x, y) \in \mathcal{D}} p(x, y) \log(p(x, y))    
\]
и да бъде консистентно с ограниченията наложени от двата класификатора. Ако с $h_1$ бележим класификатора на реч

\end{document}
