\documentclass[main.tex]{subfiles}

\begin{document}

В предните раздели описахме получаването на класификатори за сигнали от реч и ЕЕГ поотделно. Сега въпросът е дали можем да съчетаем по някакъв начин данните или класификаторите с цел да получим по-добра класификационна точност. Разгледаните подходи са два. Първият е наивен и съчетава самите характеристични вектори, а вторият - вече получените класификатори. 

\section{Съчетаване чрез конкатенация на характеристичните вектори}
\subsection{Описание}
Тук идеята е възможно най-проста - конкатенираме характеристичните вектори от речта и тези от ЕЕГ сигнала до получаване на нов вектор и тренираме класификатора, описан в \autoref{chap:em}. Единствената трудност е, че векторите за реч се взимат много по-често. За да има еднакъв брой вектори за двата сигнала, тези за речта се осредняват на всеки 200 ms.
\subsection{Резултати}
На таблица \autoref{tab:double:results:01} е показана средната класификационна точност за всяка една от емоциите за двата класификатора поотделно и накрая за този, получен при конкатенацията на векторите. Всяка от Гаусовите смески на класификатора на реч има 8 гаусиани, докато този за ЕЕГ и полученият при конкатенация имат по 3 гаусиани. От таблицата се вижда, че този метод не довежда до подобрение. Това вероятно се дължи на малкото количество данни, тъй като векторите, с които работим, са $39 + 76 = 115$ мерни. Това означава, че за да видим повече проявления на характеристиките, ще са нужни много повече данни. Освен набавянето на допълнително данни, може да се намали пространството чрез факторен анализ, за да се подобри обучението на класификатора.

\begin{table}[h]
    \begin{center}
    \begin{tabular}{|l|r r r|} 
        \hline
        Емоция & Само реч & Само ЕЕГ & Конкатенация \\ 
        \hline
        Гняв & 85.00\% & 80.00\% & 85.00\%\\ 
        Щастие & 75.00\% & 75.00\% & 50.00\%\\ 
        Неутрално & 7.50\% & 97.50\% & 97.50\%\\ 
        Тъга & 85.42\% & 87.50\% & 87.50\%\\ 
        \hline
        \hline
        Общо & \textbf{63.23\%} & \textbf{85.00\%} &  \textbf{80.00\%}\\
        \hline
    \end{tabular}
    \caption{Класификационна точност на класификатор за реч, класификатор за ЕЕГ и класификатор, получен при конкатенация на характеристичните вектори}
    \label{tab:double:results:01}
    \end{center}
\end{table}

\section{Съчетаване чрез AdaBoost}
\subsection{Описание}
Това, което имаме са два ``слаби'' класификатора, които искаме да комбинираме. Полученият класификатор е линейна комбинация на двата слаби, а теглата се намират чрез AdaBoost алгоритъма. В оригиналния си вариант, описан в сдфсдф, се говори за бинарни класификатори, но тъй като в случая имаме повече от два класа, използваме модификацията от \cite{samme}. Нека имаме тренировъчни примери $(x_1, c_1)\cdots (x_n, c_n)$, където $x_i \in \mathbb{R}^p, c_i \in \mathbb{N}, 1\leq c_i \leq K$, а $K$ е броят класове. Множеството на слабите класификатори ще означаваме с $\mathcal{H}$, като всяко $h_i \in \mathcal{H}$ по вход $x$ определя класа му $c$. Нека $T$ е константа, която означава броят итерации на алгоритъма. Тогава търсеният класификатор е: \[H(x) = \sum\limits_{i=1}^{T} \alpha_i h_i(x)\]
Алгоритъмът AdaBoost избира на всяка стъпка класификатор измежду наличните $|\mathcal{H}|$ и намира съответното му теглото $\alpha$. Данните се взимат с тегла, които отразяват доколко текущо-взетите класификатори са сбъркали върху тях. Следващият класификатор, който се избира, е такъв, който се представя добре върху примерите, които останалите класификатори бъркат. Тъй като се избира класификатор, който минимизира грешката върху претеглените данни (претеглена грешка), то и полученият класификатор $H$ ще има минимална грешка върху тренировъчното множество (както също може да се види в сдфсдф). Модифицираният AdaBoost алгоритъм е следният:


\begin{exampleenv}
\begin{tabular}{p{0.65\textwidth}|p{0.30\textwidth}}
    \makecell[l]{1. Инициализираме първоначалните тегла\\$w_i = \cfrac{1}{n}, i = 1, 2,\cdots n$} & \footnotesize{$n$ е броят на тренировъчните примери}\\
    & \\
    2. За всяко $t$ от 1 до $T$ се прави следното: & \\
    \qquad (а) Избираме $h_t = argmin_{h \in \mathcal{H}}\cfrac{\sum\limits_{i=1}^n w_i \mathbb{1}(h(x_i) \neq c_i)}{\sum\limits_{i=1}^n w_i}$ & \footnotesize{Избираме това $h$, което минимизира грешката на така претеглените данни}\\
    \makecell[l]{\qquad (б) Пресмятаме грешката\\\qquad $\varepsilon_t = \cfrac{\sum\limits_{i=1}^n w_i \mathbb{1}(h(x_i) \neq c_i)}{\sum\limits_{i=1}^n w_i}$} & \\
    \makecell[l]{\qquad (в) Пресмятаме теглото\\\qquad $\alpha_t = \log\cfrac{1 - \varepsilon_t}{\varepsilon_t}$} & \\
    \makecell[l]{\qquad (г) Обновяваме теглата\\\qquad $w_i\prime = \cfrac{w_i exp(\alpha_t \mathbb{1}(h_t(x_i) \neq c_i))}{\sum\limits_{j=1}^n w_j exp(\alpha_t \mathbb{1}(h_t(x_j) \neq c_j))}$\\\qquad$w_i = w_i\prime, i = 1\cdots n$} & \\
    3. Изход $H(x) = argmax_c \sum\limits_{t=1}^T \alpha_t \mathbb{1}(h_t(x) = c)$ & \footnotesize{или ако $h_t(x, c)$ връща вероятността $x$ да е от клас $c$, то $H(x) = argmax_c \sum\limits_{t=1}^T \alpha_t h_t(x, c)$}
\end{tabular}
\end{exampleenv}

В случая, имаме само два класификатора. Въпреки това, се извършват повече от две итерации, като на всяка стъпка се избира някой от двата класификатора и тежестта му $\alpha$ се обновява.

\subsection{Резултати}

Първият опит е да се провери дали AdaBoost допринася за класификационната точност на ниво файл. Резултатите са показани на \autoref{tab:double:results:02}

\begin{table}[h]
    \begin{center}
    \begin{tabular}{|l|r r r|} 
        \hline
        Емоция & Само реч & Само ЕЕГ & AdaBoost \\ 
        \hline
        Гняв & 85.00\% & 80.00\% & 80.00\%\\ 
        Щастие & 75.00\% & 75.00\% & 75.00\%\\ 
        Неутрално & 7.50\% & 97.50\% & 97.50\%\\ 
        Тъга & 85.42\% & 87.50\% & 87.50\%\\ 
        \hline
        \hline
        Общо & \textbf{63.23\%} & \textbf{85.00\%} &  \textbf{85.00\%}\\
        \hline
    \end{tabular}
    \caption{Класификационна точност на ниво файл на класификатор за реч, класификатор за ЕЕГ и класификатор, получен чрез AdaBoost от двата класификатора}
    \label{tab:double:results:02}
    \end{center}
\end{table}

От тях се вижда, че полученият класификатор има същата класификационна точност като този за ЕЕГ. При анализ на резултатите се вижда, че макар че в наличните данни има файл, на който класификаторът на реч бърка, но този за ЕЕГ познава, няма свидетел за обратното. AdaBoost се научава да вярва много повече на класификатора за ЕЕГ, което се отразява в разликата на $\alpha$-те - теглото на класификатора за ЕЕГ е тройно по-голямо от това на класификатора за реч. 

Оказва се, че ако разглеждаме класификацията на всеки вектор поотделно, то в данните има такъв вектор, за който класификаторът на реч познава, но този за ЕЕГ - не.
Можем да видим ефекта на AdaBoost, ако разглеждаме класификацията на ниво вектор. Резултатите са показани в \autoref{tab:double:results:03}.
\begin{table}[h]
    \begin{center}
    \begin{tabular}{|l|r r r|} 
        \hline
        Емоция & Само реч & Само ЕЕГ & AdaBoost \\ 
        \hline
        Гняв &  41.53\% & 53.05\% & 53.37\%\\ 
        Щастие &  37.35\% & 44.44\%  & 43.73\%\\ 
        Неутрално & 32.91\% & 66.72\%  & 66.67\%\\ 
        Тъга & 39.66 \% & 70.38\% & 70.87\%\\ 
        \hline
        \hline
        Общо & \textbf{37.86\%} & \textbf{58.65\%} &  \textbf{58.66\%}\\
        \hline
    \end{tabular}
    \caption{Класификационна точност на ниво вектор на класификатор за реч, класификатор за ЕЕГ и класификатор, получен чрез AdaBoost от двата класификатора}
    \label{tab:double:results:03}
    \end{center}
\end{table}

От тях се вижда минимално подобрение от $0.01\%$, което е недостатъчно да промени резултата на ниво файл. 
За да можем да научим комбиниран класификатор от двата налични с класификационна точност по-голяма от тази на по-добрия от двата, ни трябват свидетели за ситуации, в които единият класификатор бърка, а другият познава, и обратното. В наличните данни няма достатъчно пример за посоката, в която класификаторът за реч познава, а този за ЕЕГ бърка, затова и полученият комбиниран класификатор не донася подобрение. 

\end{document}
